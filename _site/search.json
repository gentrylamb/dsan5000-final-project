[
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nOnce we decided on a topic, ChatGPT was used to help with refining our research questions and narrow in on topics for related work.1"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nProofreading and improving grammar in the literature reviews.2\nImproved content we wrote to improve overall flow and idea expression3"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nUsed ChatGPT to help with error that was faced when accessing the ChromeDriver for Selenium. It suggested that we add the 4 lines of code adjusting chrome options.4"
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Summary",
    "text": "Summary\nWashington DC is a leader in urban cycling infrastructure, with an expansive network of bike lanes and trails supporting eco-friendly commuting. However, DC’s trails are not only for commuting within the city but also numerous options for purely recreational use. In the confides of a large city these options for recreation can be extremly popular, so a deeper understanding of bike route utilization, accessibility, and safety are imperative. This project focuses on scraping, analyzing, and visualizing data about DC’s bike routes to identify patterns and trends through clustering and classification. The insights gained can guide city planners in optimizing infrastructure, improving safety, and promoting cycling as a sustainable mode of transportation."
  },
  {
    "objectID": "index.html#significance",
    "href": "index.html#significance",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Significance",
    "text": "Significance\nCycling is integral to sustainable urban mobility, offering benefits such as reduced carbon emissions, improved public health, and decreased traffic congestion. By analyzing cycling routes, we can uncover patterns of use, gaps in infrastructure, and safety concerns, enabling data-driven decision-making for a greener and safer DC."
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Intended Audience",
    "text": "Intended Audience\n\nBike enthusiasts in Washington, DC\nData professionals looking to replicate analysis in other cities"
  },
  {
    "objectID": "index.html#key-topics",
    "href": "index.html#key-topics",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Key Topics",
    "text": "Key Topics\n\nKMeans Clustering\nSpectral Clustering\nRegression Decision Tree Classificaiton\nPredictive Models\nData Visualization"
  },
  {
    "objectID": "index.html#related-work",
    "href": "index.html#related-work",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Related Work",
    "text": "Related Work\n\nUrban Cycling Studies: Research linking bike infrastructure to cycling adoption and safety.\nGeospatial Data Science: Use of clustering algorithms for route categorization and optimization in other urban contexts.\nMachine Learning in Mobility: Classification techniques applied to transportation data to understand user behavior and needs.\nLocal Reports: Existing reports on cycling trends in Washington, DC, such as those from DDOT and Capital Bikeshare."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Research Questions",
    "text": "Research Questions\n\nHow can clustering techniques help categorize bike routes based on usage, distance, location, or ratings?\nWhich factors are the most important in clustering of bike routes?\nIs it possible to classify bike routes with existing data?\nWhat insights can be drawn about the existing routes in developing more routes as the city grows?\nWhat data would make this clustering and classification more robust?"
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "Literature Review",
    "text": "Literature Review\nIn the past decade, urban cycling infrastructure has garnered attention due to the relevance in addressing environmental sustainability concerns. Cycling offers a low-cost, sustainable mode of transportation that reduces greenhouse gas emissions, mitigates traffic congestion, and promotes physical activity. Understanding the current landscape of research on this topic is crucial for identifying gaps in knowledge and expand on them. This literature review aims to harmonize key findings, methodologies, and trends have shaped the current understanding of this topic area.\n“How to Use Selenium to Web-Scrape with Example”1 provides a step-by-step guide on using Selenium, a Python library, for web scraping tasks, focusing on extracting NBA player salary data from Hoopshype. Selenium automates web browsers, allowing for efficient data collection from websites that may be otherwise difficult to scrape through traditional methods. Selenium’s advantage lies in its ability to interact with dynamic web pages, which may load content via JavaScript. This method is particularly useful for collecting structured data from websites that do not provide APIs or straightforward access to information, such as the site we are interested in gathering our data from. This article is relevant to our study as it demonstrates the basic workflow for setting up and using Selenium for web scraping. The same techniques of web driver installation, XPath usage, and data extraction will be applicable for scraping bike route information from the provided websites. Additionally, the iterative process of scraping multiple pages and aggregating data aligns with the need to collect comprehensive datasets for clustering and classification in this project.\nIn the article “Visual Exploration of Cycling Semantics with GPS Trajectory Data”2, the authors address the need for a comprehensive system to analyze cycling semantics from both the cyclist’s and the road’s perspectives. By utilizing large-scale GPS trajectory and road network data, the authors propose VizCycSemantics, a visual analytic system that aims to uncover hidden patterns in cycling behaviors and moving characteristics along road segments. While this paper gets bogged down with technical jargon about GPS data and challenges, they present some very interesting points on clustering. The VizCycSemantics system uses Latent Dirichlet Allocation (LDA) to extract cycling topics—representing cyclist behaviors—and k-means++ clustering to identify groups of similar cyclists and road segments. This research is particularly relevant to our project, as it provides a proven framework for analyzing cycling data through clustering and topic modeling. While our base approach will differ significantly, our goal of uncovering patterns in cycling routes and behaviors aligns well with their study.\nChapter 9 of “Capital Dilemma”3 addresses bicycling trends and policies in the DC area since 1990. The study of cycling trends in the Washington, DC metropolitan area provides essential insights into the growing role of cycling in urban mobility and infrastructure planning. The expansion of bike lanes, along with innovative programs and policies, has significantly increased cycling levels in the region, particularly within the urban core of Washington, DC, Arlington, and Alexandria. As highlighted, cycling levels have risen in tandem with the development of bike-friendly infrastructure, such as on-street bike lanes and off-street shared-use paths, though challenges remain in terms of accessibility and inclusivity. This literature is highly relevant to our project, where we aim to analyze and classify bike routes in Washington, DC. Understanding the spatial distribution of bike lanes and their relationship with cyclist behavior in various neighborhoods offers a valuable framework for our clustering and classification efforts.\nThe article “Machine Learning Approaches to Bike-Sharing Systems: A Systematic Literature Review”4 details numerous applications of machine learning techniques within the sphere of bike-sharing systems (BSS). It stresses the importance of these systems in the modern era, highlighting their role in reducing carbon emissions, improving transportation accessibility, and influencing the overall transportation culture of a city. The paper reviews 35 studies from 2015 to 2019, aiming to evaluate the performance of various machine learning techniques applied to BSS while also identifying challenges and proposing future research directions. The methodology used to evaluate the machine learning methods is the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) Framework. Key findings from the paper include the high accuracy with which random forests and gradient boosting machines predicted bike availability and demand trends. Other techniques, such as decision trees and deep learning, were useful for understanding trip patterns and user segmentation. Deep learning, in particular, is becoming more prevalent, as neural networks are capable of efficiently handling complex datasets. Finally, clustering techniques were applied to analyze station usage patterns and trends. The paper concludes that, through a combination of machine learning, transportation engineering, and planning, future efforts should focus on developing machine learning models that are easily interpretable and transferable to various cities. Another goal is to leverage real-time data to optimize BSS instantaneously.\nThe article “Learning to Cluster Urban Areas: Two Competitive Approaches and an Empirical Validation”5 compares two clustering methodologies to effectively group urban areas with similar characteristics. The two approaches introduced are Deep Modularity Networks (DMON) and a traditional graph-based clustering method. DMON is a neural network approach designed to optimize cluster modularity. Specifically, it graphically represents urban areas and connects them to other areas that share similar spatial or economic traits. The network maximizes modularity while adapting to different situations for greater flexibility. The traditional graph method, on the other hand, relies on classical algorithms like spectral clustering. Although these algorithms also maximize modularity, they underperform in terms of adaptability, scalability, and other areas. Through empirical validation, the authors demonstrate that DMON outperforms the traditional graph method in terms of performance, processing efficiency, and utility. One application of DMON discussed in the paper is its use for transportation planning. This clustering method helps identify critical transportation nodes and corridors, facilitating more efficient transit design. However, DMON has some limitations, particularly in terms of interpretability. Deep learning often produces outputs that are difficult for non-experts to understand, which could hinder the urban planning process and lead to stagnation. Despite this, the paper demonstrates the effectiveness of clustering methods in addressing urban environmental issues.\nUnlike the previous two papers, this research, “A Taxonomy of Machine Learning Clustering Algorithms, Challenges, and Future Realms”6, offers a more comprehensive examination of clustering algorithms. The paper categorizes various clustering algorithms based on their methodologies and use cases, providing insight into which algorithms are best suited for specific situations. More specifically, the paper focuses on five different types of clustering algorithms: partitioning-based clustering, hierarchical clustering, density-based clustering, model-based clustering, and graph-based clustering. Partitioning-based clustering is best used in applications such as customer segmentation and urban planning. Hierarchical clustering is more appropriate for social network analysis or gene expression studies. Density-based clustering is suitable for spatial analysis and transportation planning. Model-based clustering is commonly applied to problems involving images and text. Finally, graph-based clustering excels in social network detection and geospatial analysis. Regardless of the problem, all clustering methods have their inherent limitations and should be used carefully. Future directions for clustering algorithms include combining two or more algorithms to leverage their strengths, as well as exploring automatic hyperparameter tuning to ensure optimal performance for specific problems. Clustering is an invaluable tool for solving real-world problems, but it is essential to choose the right algorithm based on the specific context."
  },
  {
    "objectID": "index.html#about-us",
    "href": "index.html#about-us",
    "title": "Analyzing Bike Routes in Washington, DC",
    "section": "About Us",
    "text": "About Us\n\n\n\nGentry Lamb\nGentry Lamb is a Graduate Student at Georgetown University pursuing a Master’s in Data Science and Analytics. He received a Bachelor’s in Operations Research in 2024 from the United States Air Force Academy in Colorado Springs, CO. He was also commissioned into the U.S. Air Force as a 2nd Lt and will be attending Pilot Training at Vance AFB in Oklahoma after graduating from Georgetown. After his time in the Air Force, he looks forward to working as a data scientist or operations analyst.\nEducation: - 2024: United States Air Force Academy (USAFA) - B.S. Operations Research\n- 2025: Georgetown University - M.S. Data Science and Analytics (Current)\n\n\n\nGentry Lamb\n\n\n\n\n\n\n\nChase Clemence\nChase Clemence is currently a graduate student at Georgetown University pursuing a Master’s of Science in Data Science & Analytics. He received a Bachelor’s of Science in Operations Research from the United States Air Force Academy in 2024, commissioning as a 2nd Lieutenant. After obtaining his degree, he will head to Goodfellow AFB in San Angelo, Texas, to start training as a Military Intelligence Officer in the United States Air Force. Following his Air Force career, he aspires to earn an MBA and work in industry. He hopes having a background in data science and business will make him an effective leader in the civilian sector.\nEducation: - 2024: United States Air Force Academy (USAFA) - B.S. Operations Research\n- 2025: Georgetown University - M.S. Data Science and Analytics (Current)\n\n\n\nChase Clemence"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The primary objective of this project is to leverage data from bikewashington.org to provide actionable insights about bike paths in Washington, DC. By analyzing and presenting detailed information about these trails, we aim to help riders make informed decisions about their routes. This knowledge will enhance their preparation and contribute to safer biking experiences across the city."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals",
    "href": "technical-details/data-collection/overview.html#goals",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The primary objective of this project is to leverage data from bikewashington.org to provide actionable insights about bike paths in Washington, DC. By analyzing and presenting detailed information about these trails, we aim to help riders make informed decisions about their routes. This knowledge will enhance their preparation and contribute to safer biking experiences across the city."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#motivation",
    "href": "technical-details/data-collection/overview.html#motivation",
    "title": "DSAN-5000: Project",
    "section": "Motivation",
    "text": "Motivation\nOur motivation lies in supporting the biking community by empowering riders to choose trails that align with their specific needs and goals. Whether for exercise, commuting, or leisure, informed decisions about bike paths can optimize the overall experience. Additionally, providing better trail information is expected to inherently enhance safety, especially for those exploring new or unfamiliar routes."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "DSAN-5000: Project",
    "section": "Objectives",
    "text": "Objectives\n\nTrail Clustering: Group similar bike trails in Washington, DC, using algorithms such as KMeans Clustering and Spectral Clustering.\nLength Prediction: Develop a regression decision tree model to predict the length of a trail based on its other qualities.\nPersonalized Recommendations: Offer tailored trail recommendations for riders of varying experience levels, addressing their unique preferences."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#data-source-information",
    "href": "technical-details/data-collection/overview.html#data-source-information",
    "title": "DSAN-5000: Project",
    "section": "Data Source Information",
    "text": "Data Source Information\nAll data used in this project originates from Bike Washington. The website provides a table of detailed information about bike routes within the DMV (DC, Maryland, and Virginia) area. Specifically, it includes:\n\nTrail Length: Measured in miles.\nTrail Name: The name of each bike route.\nTrail Description: A summary of the trail’s features.\nRatings: Scores for terrain difficulty, traffic levels, and scenic beauty.\n\n\nRating Definitions\n\nTerrain: A score of 1 corresponds to a very flat trail, while 5 represents extremely challenging climbs.\nTraffic: A score of 1 indicates minimal traffic, while 5 suggests heavy traffic conditions.\nScenery: A score of 1 indicates a highly scenic trail, whereas 5 represents less visually appealing routes."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#data-collection-methods",
    "href": "technical-details/data-collection/overview.html#data-collection-methods",
    "title": "DSAN-5000: Project",
    "section": "Data Collection Methods",
    "text": "Data Collection Methods\nWe utilized Selenium, an open-source browser automation framework, to scrape data from the website. Specifically: - The webdriver function was employed to extract trail names, mileage, and ratings. - The extracted data was processed using Pandas to consolidate all relevant information into a single DataFrame. - The tabular data was then saved for future analysis."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#data-structure-and-format",
    "href": "technical-details/data-collection/overview.html#data-structure-and-format",
    "title": "DSAN-5000: Project",
    "section": "Data Structure and Format",
    "text": "Data Structure and Format\nThe cleaned data was exported to a CSV file consisting of: - Rows: 43 (one for each trail). - Columns: 6 (trail name, description, mileage, terrain rating, traffic rating, and scenery rating). - Data Types: - string for trail names and descriptions. - integer for mileage and ratings. - Regression Target: mileage - Binary Classification Target: insert here - Multiclass-Classification Target: location of trail (Maryland, Virginia, or DC) - found in the description"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#methods",
    "href": "technical-details/data-collection/overview.html#methods",
    "title": "DSAN-5000: Project",
    "section": "Methods",
    "text": "Methods\nWe utilized Selenium, an open-source framework for automating web browsers, to scrape data from bikewashington.org. The scraped information included trail names, descriptions, lengths, and ratings, which were initially stored in lists and later consolidated into a single Pandas DataFrame.\nThe ratings were further processed and split into three distinct columns: terrain, scenery, and traffic. Irrelevant or redundant columns were removed from the dataset to streamline the analysis. One such column was the original ‘rating’ column. Finally, the cleaned DataFrame was exported as a CSV file for seamless integration with the machine learning components of this project.\nWhile the dataset is relatively small, it encompasses all the essential information about bike paths and trails in the DC area, making it a valuable resource for our analysis."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html",
    "href": "technical-details/data-cleaning/overview.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Cleaning the raw trail data was a critical step in extracting meaningful insights. The descriptions of each bike trail contained a wealth of information, making data cleaning essential for conducting deeper exploratory analysis and enabling robust machine learning applications. It’s important to note that data cleaning is a dynamic and iterative process. As this project evolves, additional cleaning may be required. Below is a summary of the transformations applied to produce the cleaned dataset."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#overview",
    "href": "technical-details/data-cleaning/overview.html#overview",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Cleaning the raw trail data was a critical step in extracting meaningful insights. The descriptions of each bike trail contained a wealth of information, making data cleaning essential for conducting deeper exploratory analysis and enabling robust machine learning applications. It’s important to note that data cleaning is a dynamic and iterative process. As this project evolves, additional cleaning may be required. Below is a summary of the transformations applied to produce the cleaned dataset."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#explanation-of-new-variables",
    "href": "technical-details/data-cleaning/overview.html#explanation-of-new-variables",
    "title": "DSAN-5000: Project",
    "section": "Explanation of New Variables",
    "text": "Explanation of New Variables\n\nUnpaved: Binary variable (1 = unpaved road, 0 = paved road).\nFlat: Binary variable (1 = flat road, 0 = not flat).\nWorkout: Binary variable (1 = trail designed for workouts, 0 = not designed for workouts).\nPark: Binary variable (1 = trail located in a park, 0 = not located in a park).\nRiver: Binary variable (1 = trail runs along a river, 0 = does not run along a river).\nLoop: Binary variable (1 = trail forms a loop, 0 = does not form a loop).\nSentiment: Float variable measuring the sentiment score of the trail description.\nState1: The state where the trail begins.\nState2: The state where the trail ends."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#managing-missing-data",
    "href": "technical-details/data-cleaning/overview.html#managing-missing-data",
    "title": "DSAN-5000: Project",
    "section": "Managing Missing Data",
    "text": "Managing Missing Data\nThis dataset initially had no missing values—every cell contained data. To confirm, we used the is.na().sum() function in Python to verify the absence of null or missing values. When text-scraping trail descriptions to generate binary variables (e.g., identifying if the word “workout” was present), missing words were assumed to indicate the absence of that characteristic, and the corresponding cell was assigned a value of 0."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#outlier-detection-and-treatment",
    "href": "technical-details/data-cleaning/overview.html#outlier-detection-and-treatment",
    "title": "DSAN-5000: Project",
    "section": "Outlier Detection and Treatment",
    "text": "Outlier Detection and Treatment\nOutliers were not a significant concern. For instance, while the C&O Towpath trail spans 184 miles—considerably longer than most other trails—removing such outliers was unnecessary. Given the limited number of bike trails in the DMV area, retaining all observations was crucial for maintaining data integrity and size."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#data-type-corrections-and-formatting",
    "href": "technical-details/data-cleaning/overview.html#data-type-corrections-and-formatting",
    "title": "DSAN-5000: Project",
    "section": "Data Type Corrections and Formatting",
    "text": "Data Type Corrections and Formatting\nThe raw dataset contained two data types: object (trail names and descriptions) and int64 (ratings and mileage). After cleaning, the dataset included object, int64, and float64 types:\n\nObject: name, state1, state2.\nFloat64: sentiment.\nInt64: All other features.\n\nDescriptions were transformed to lowercase for consistency and ease of keyword matching. The most significant transformation involved converting descriptions into multiple binary (one-hot-encoded) features based on meaningful keywords, as described in the “New Variables” section. State information was also extracted using lists of specific locations within Virginia, Maryland, DC, and Pennsylvania, allowing us to assign values to state1 and state2. This process enabled the extraction of numerical data from text, enriching the dataset for subsequent machine learning analyses."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#normalization-and-scaling",
    "href": "technical-details/data-cleaning/overview.html#normalization-and-scaling",
    "title": "DSAN-5000: Project",
    "section": "Normalization and Scaling",
    "text": "Normalization and Scaling\nGiven the significant right skew in the mileage distribution, normalization was applied. We used z-score normalization to scale this variable, ensuring it would not disproportionately influence machine learning models. Both normalized and unnormalized mileage distributions are provided in the ‘Code’ section below."
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html#subsetting-data",
    "href": "technical-details/data-cleaning/overview.html#subsetting-data",
    "title": "DSAN-5000: Project",
    "section": "Subsetting Data",
    "text": "Subsetting Data\nSubsetting was deemed unnecessary due to the already limited size of the dataset."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#regression-tree",
    "href": "technical-details/supervised-learning/main.html#regression-tree",
    "title": "Supervised Learning",
    "section": "Regression Tree",
    "text": "Regression Tree\n\n## Make a Regression Tree that predicts the sentiment of a bike path given mileage and other binary variables\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../../data/processed-data/dc_bike_routes.csv')\nY = df['sentiment']\nX = df.iloc[:, 2:12]\n\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\ny_train = y_train\ny_test = y_test\n\nhyper_parameters = []\ntrain_error = []\ntest_error = []\n\n# Optimize Hyperparameter\nfor value in range(1,20):\n\n    model = DecisionTreeRegressor(max_depth = value)\n\n    model.fit(x_train, y_train)\n\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    train_error_sample = mean_squared_error(y_train, yp_train)\n    test_error_sample = mean_squared_error(y_test, yp_test)\n\n    hyper_parameters.append(value)\n    train_error.append(train_error_sample)\n    test_error.append(test_error_sample)\n\n# Plot the data\nplt.plot(hyper_parameters, train_error, label=\"Training Error\", color=\"blue\", linestyle=\"-\", marker=\"o\")\nplt.plot(hyper_parameters, test_error, label=\"Testing Error\", color=\"red\", linestyle=\"--\", marker=\"x\")\n\n# Add labels and title\nplt.xlabel(\"Hyperparameter Range\")\nplt.ylabel(\"Mean Squared Error (Blue = Training & Red = Testing)\")\nplt.title(\"Line Plot of Training and Testing Error\")\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Make the best model and output the errors\nmodel_best = DecisionTreeRegressor(max_depth = 2)\n\nmodel_best.fit(x_train, y_train)\n\nyp_train = model_best.predict(x_train)\nyp_test = model_best.predict(x_test)\n\ntrain_error_best = mean_squared_error(y_train, yp_train)\ntest_error_best = mean_squared_error(y_test, yp_test)\n\nprint(\"Training Error:\", train_error_best)\nprint(\"Testing Error:\", test_error_best)\n\nTraining Error: 0.11066305675070029\nTesting Error: 0.047755677369614516"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#classification-trees",
    "href": "technical-details/supervised-learning/main.html#classification-trees",
    "title": "Supervised Learning",
    "section": "Classification Trees",
    "text": "Classification Trees\n\n# Functions for displaying accuracy and the model\n\n# Confusion Plot to visualize accuracy of the model later on\ndef confusion_plot(y_data, y_pred):\n    # Calculate and print accuracy\n    accuracy = accuracy_score(y_data, y_pred)\n    print(\"ACCURACY:\", accuracy)\n\n    if len(y_data.unique()) == 2:\n        neg_recall = recall_score(y_data == 0, (y_pred == 0))\n        neg_precision = precision_score((y_data) == 0, (y_pred == 0))\n        pos_recall = recall_score((y_data) == 1, (y_pred == 1))\n        pos_precision = precision_score((y_data) == 1, (y_pred == 1))\n\n        print(\"NEGATIVE RECALL (Y=0):\", neg_recall)\n        print(\"NEGATIVE PRECISION (Y=0):\", neg_precision)\n        print(\"POSITIVE RECALL (Y=1):\", pos_recall)\n        print(\"POSITIVE PRECISION (Y=1):\", pos_precision)\n\n    conf_matrix = confusion_matrix(y_data, y_pred)\n    print(conf_matrix)\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n    disp.plot()\n    plt.show()\n\n# Display a tree\ndef display_tree(model,X,Y):\n    fig = plt.figure(figsize=(25,20))\n    plot_tree(model,\n                feature_names=X.columns,\n                class_names=[str(c) for c in Y.unique()],\n                filled=True)\n\n    plt.show()\n\n\n# Try to predict loop based on other variables. FIrst, let's optimize the hyper parameters\nY_class_bin = df['loop']\nX_class_bin = pd.concat([df.iloc[:, 2:11], df['sentiment']], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(X_class_bin, Y_class_bin, test_size = 0.2, random_state = 0)\n\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,15):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 20))\n\naxes[0].plot(range(1,15),[result[1] for result in test_results],'-or')\naxes[0].plot(range(1,15),[result[1] for result in train_results],'-ob')\naxes[0].set_xlabel('Number of layers in decision tree (max_depth)')\naxes[0].set_ylabel('ACCURACY: Training (blue) and Test (red)')\n\naxes[1].plot(range(1,15),[result[2] for result in test_results],'-or')\naxes[1].plot(range(1,15),[result[2] for result in train_results],'-ob')\naxes[1].set_xlabel('Number of layers in decision tree (max_depth)')\naxes[1].set_ylabel('RECALL (Y=0): Training (blue) and Test (red)')\n\naxes[2].plot(range(1,15),[result[3] for result in test_results],'-or')\naxes[2].plot(range(1,15),[result[3] for result in train_results],'-ob')\naxes[2].set_xlabel('Number of layers in decision tree (max_depth)')\naxes[2].set_ylabel('RECALL (Y=1): Training (blue) and Test (red)')\n\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\n\n\n# Build the optimal model and display the confusion matrix to show accuracy. Display the tree\nmodel_best = DecisionTreeClassifier(max_depth = 7)\nmodel_best = model_best.fit(x_train, y_train)\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\ndisplay_tree(model_best, X_class_bin, Y_class_bin)\n\n------TRAINING------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[25  0]\n [ 0  9]]\n\n\n\n\n\n\n\n\n\n------TEST------\nACCURACY: 0.7777777777777778\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 0.75\nPOSITIVE RECALL (Y=1): 0.3333333333333333\nPOSITIVE PRECISION (Y=1): 1.0\n[[6 0]\n [2 1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Now let's build a classification tree to try and predict the state in which a bike trail starts in given all numerical/binary features\nY_class = df['state1']\nX_class = df.iloc[:, 2:13]\n\nx_train, x_test, y_train, y_test = train_test_split(X_class, Y_class, test_size = 0.2, random_state = 0)\n\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,15):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    test_results.append([num_layer,accuracy_score(y_test, yp_test)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train)])\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n\naxes.plot(range(1,15),[result[1] for result in test_results],'-or')\naxes.plot(range(1,15),[result[1] for result in train_results],'-ob')\naxes.set_xlabel('Number of layers in decision tree (max_depth)')\naxes.set_ylabel('ACCURACY: Training (blue) and Test (red)')\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\n\n\n# Build the optimal model and display the confusion matrix to show accuracy. Display the tree\nmodel_best = DecisionTreeClassifier(max_depth = 2)\nmodel_best = model_best.fit(x_train, y_train)\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\ndisplay_tree(model_best, X_class, Y_class)\n\n------TRAINING------\nACCURACY: 1.0\n[[10  0  0  0]\n [ 0 18  0  0]\n [ 0  0  1  0]\n [ 0  0  0  5]]\n\n\n\n\n\n\n\n\n\n------TEST------\nACCURACY: 0.4444444444444444\n[[0 1 0]\n [2 4 1]\n [0 1 0]]"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "The primary objective of this project is to leverage data from bikewashington.org to provide actionable insights about bike paths in Washington, DC. By analyzing and presenting detailed information about these trails, we aim to help riders make informed decisions about their routes. This knowledge will enhance their preparation and contribute to safer biking experiences across the city."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "",
    "text": "The primary objective of this project is to leverage data from bikewashington.org to provide actionable insights about bike paths in Washington, DC. By analyzing and presenting detailed information about these trails, we aim to help riders make informed decisions about their routes. This knowledge will enhance their preparation and contribute to safer biking experiences across the city."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "Motivation",
    "text": "Motivation\nOur motivation lies in supporting the biking community by empowering riders to choose trails that align with their specific needs and goals. Whether for exercise, commuting, or leisure, informed decisions about bike paths can optimize the overall experience. Additionally, providing better trail information is expected to inherently enhance safety, especially for those exploring new or unfamiliar routes."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "Objectives",
    "text": "Objectives\n\nTrail Clustering: Group similar bike trails in Washington, DC, using algorithms such as KMeans Clustering and Spectral Clustering.\nLength Prediction: Develop a regression decision tree model to predict the length of a trail based on its other qualities.\nPersonalized Recommendations: Offer tailored trail recommendations for riders of varying experience levels, addressing their unique preferences."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-source-information",
    "href": "technical-details/data-collection/main.html#data-source-information",
    "title": "Data Collection",
    "section": "Data Source Information",
    "text": "Data Source Information\nAll data used in this project originates from Bike Washington. The website provides a table of detailed information about bike routes within the DMV (DC, Maryland, and Virginia) area. Specifically, it includes:\n\nTrail Length: Measured in miles.\nTrail Name: The name of each bike route.\nTrail Description: A summary of the trail’s features.\nRatings: Scores for terrain difficulty, traffic levels, and scenic beauty.\n\n\nRating Definitions\n\nTerrain: A score of 1 corresponds to a very flat trail, while 5 represents extremely challenging climbs.\nTraffic: A score of 1 indicates minimal traffic, while 5 suggests heavy traffic conditions.\nScenery: A score of 1 indicates a highly scenic trail, whereas 5 represents less visually appealing routes."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-methods",
    "href": "technical-details/data-collection/main.html#data-collection-methods",
    "title": "Data Collection",
    "section": "Data Collection Methods",
    "text": "Data Collection Methods\nWe utilized Selenium, an open-source browser automation framework, to scrape data from the website. Specifically: - The webdriver function was employed to extract trail names, mileage, and ratings. - The extracted data was processed using Pandas to consolidate all relevant information into a single DataFrame. - The tabular data was then saved for future analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-structure-and-format",
    "href": "technical-details/data-collection/main.html#data-structure-and-format",
    "title": "Data Collection",
    "section": "Data Structure and Format",
    "text": "Data Structure and Format\nThe cleaned data was exported to a CSV file consisting of: - Rows: 43 (one for each trail). - Columns: 6 (trail name, description, mileage, terrain rating, traffic rating, and scenery rating). - Data Types: - string for trail names and descriptions. - integer for mileage and ratings. - Regression Target: mileage - Binary Classification Target: insert here - Multiclass-Classification Target: location of trail (Maryland, Virginia, or DC) - found in the description"
  },
  {
    "objectID": "technical-details/data-collection/main.html#methods",
    "href": "technical-details/data-collection/main.html#methods",
    "title": "Data Collection",
    "section": "Methods",
    "text": "Methods\nWe utilized Selenium, an open-source framework for automating web browsers, to scrape data from bikewashington.org. The scraped information included trail names, descriptions, lengths, and ratings, which were initially stored in lists and later consolidated into a single Pandas DataFrame.\nThe ratings were further processed and split into three distinct columns: terrain, scenery, and traffic. Irrelevant or redundant columns were removed from the dataset to streamline the analysis. One such column was the original ‘rating’ column. Finally, the cleaned DataFrame was exported as a CSV file for seamless integration with the machine learning components of this project.\nWhile the dataset is relatively small, it encompasses all the essential information about bike paths and trails in the DC area, making it a valuable resource for our analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Cleaning the raw trail data was a critical step in extracting meaningful insights. The descriptions of each bike trail contained a wealth of information, making data cleaning essential for conducting deeper exploratory analysis and enabling robust machine learning applications. It’s important to note that data cleaning is a dynamic and iterative process. As this project evolves, additional cleaning may be required. Below is a summary of the transformations applied to produce the cleaned dataset."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#overview",
    "href": "technical-details/data-cleaning/main.html#overview",
    "title": "Data Cleaning",
    "section": "",
    "text": "Cleaning the raw trail data was a critical step in extracting meaningful insights. The descriptions of each bike trail contained a wealth of information, making data cleaning essential for conducting deeper exploratory analysis and enabling robust machine learning applications. It’s important to note that data cleaning is a dynamic and iterative process. As this project evolves, additional cleaning may be required. Below is a summary of the transformations applied to produce the cleaned dataset."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#explanation-of-new-variables",
    "href": "technical-details/data-cleaning/main.html#explanation-of-new-variables",
    "title": "Data Cleaning",
    "section": "Explanation of New Variables",
    "text": "Explanation of New Variables\n\nUnpaved: Binary variable (1 = unpaved road, 0 = paved road).\nFlat: Binary variable (1 = flat road, 0 = not flat).\nWorkout: Binary variable (1 = trail designed for workouts, 0 = not designed for workouts).\nPark: Binary variable (1 = trail located in a park, 0 = not located in a park).\nRiver: Binary variable (1 = trail runs along a river, 0 = does not run along a river).\nLoop: Binary variable (1 = trail forms a loop, 0 = does not form a loop).\nSentiment: Float variable measuring the sentiment score of the trail description.\nState1: The state where the trail begins.\nState2: The state where the trail ends."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#managing-missing-data",
    "href": "technical-details/data-cleaning/main.html#managing-missing-data",
    "title": "Data Cleaning",
    "section": "Managing Missing Data",
    "text": "Managing Missing Data\nThis dataset initially had no missing values—every cell contained data. To confirm, we used the is.na().sum() function in Python to verify the absence of null or missing values. When text-scraping trail descriptions to generate binary variables (e.g., identifying if the word “workout” was present), missing words were assumed to indicate the absence of that characteristic, and the corresponding cell was assigned a value of 0."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#outlier-detection-and-treatment",
    "href": "technical-details/data-cleaning/main.html#outlier-detection-and-treatment",
    "title": "Data Cleaning",
    "section": "Outlier Detection and Treatment",
    "text": "Outlier Detection and Treatment\nOutliers were not a significant concern. For instance, while the C&O Towpath trail spans 184 miles—considerably longer than most other trails—removing such outliers was unnecessary. Given the limited number of bike trails in the DMV area, retaining all observations was crucial for maintaining data integrity and size."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#data-type-corrections-and-formatting",
    "href": "technical-details/data-cleaning/main.html#data-type-corrections-and-formatting",
    "title": "Data Cleaning",
    "section": "Data Type Corrections and Formatting",
    "text": "Data Type Corrections and Formatting\nThe raw dataset contained two data types: object (trail names and descriptions) and int64 (ratings and mileage). After cleaning, the dataset included object, int64, and float64 types:\n\nObject: name, state1, state2.\nFloat64: sentiment.\nInt64: All other features.\n\nDescriptions were transformed to lowercase for consistency and ease of keyword matching. The most significant transformation involved converting descriptions into multiple binary (one-hot-encoded) features based on meaningful keywords, as described in the “New Variables” section. State information was also extracted using lists of specific locations within Virginia, Maryland, DC, and Pennsylvania, allowing us to assign values to state1 and state2. This process enabled the extraction of numerical data from text, enriching the dataset for subsequent machine learning analyses."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#normalization-and-scaling",
    "href": "technical-details/data-cleaning/main.html#normalization-and-scaling",
    "title": "Data Cleaning",
    "section": "Normalization and Scaling",
    "text": "Normalization and Scaling\nGiven the significant right skew in the mileage distribution, normalization was applied. We used z-score normalization to scale this variable, ensuring it would not disproportionately influence machine learning models. Both normalized and unnormalized mileage distributions are provided in the ‘Code’ section below."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#subsetting-data",
    "href": "technical-details/data-cleaning/main.html#subsetting-data",
    "title": "Data Cleaning",
    "section": "Subsetting Data",
    "text": "Subsetting Data\nSubsetting was deemed unnecessary due to the already limited size of the dataset."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "With our processed data, we created numerous visualizations to uncover the underlying patterns in the bike trail dataset. Most of the analyses focused on univariate relationships, though we also explored some bivariate relationships, particularly between trail ratings. Additionally, we examined the potential for clustering by comparing mileage with sentiment, using the trail’s scenery score as a color-coded feature."
  },
  {
    "objectID": "technical-details/eda/main.html#overview",
    "href": "technical-details/eda/main.html#overview",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "With our processed data, we created numerous visualizations to uncover the underlying patterns in the bike trail dataset. Most of the analyses focused on univariate relationships, though we also explored some bivariate relationships, particularly between trail ratings. Additionally, we examined the potential for clustering by comparing mileage with sentiment, using the trail’s scenery score as a color-coded feature."
  },
  {
    "objectID": "technical-details/eda/main.html#univariate-analysis",
    "href": "technical-details/eda/main.html#univariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\n\nNumerical Variables\nWe analyzed the distributions of numerical variables, particularly mileage and sentiment: - Mileage: A discrete variable, categorized into bins for easier analysis. The distribution shows that most trails fall within the 10-120 mile range, with a slight right skew indicating a few longer trails (up to 175 miles). - Sentiment: A continuous variable analyzed using Kernel Density Estimation. The distribution reveals that most sentiments are positive, concentrated within the 0-0.8 range. The bimodal nature of the distribution indicates peaks around 0 and 0.6.\n\n\nCategorical Variables\nBar charts helped us visualize the distributions of trail ratings—terrain, traffic, and scenery: - Terrain: The only rating using the full range (1-5). Most trails are rated 3, with fewer trails rated 4 or 5. - Traffic: Ranges from 1-4, with most trails rated 2 or 3. - Scenery: Restricted to scores of 1-3, with the majority of trails rated 1 or 2."
  },
  {
    "objectID": "technical-details/eda/main.html#bivariate-analysis",
    "href": "technical-details/eda/main.html#bivariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nHeatmaps\nWe explored relationships between trail ratings using heatmaps: - Terrain vs. Scenery: The combination of terrain (3) and scenery (1) is the most frequent. - Terrain vs. Traffic: The most common combination is when both ratings are 3. - Traffic vs. Scenery: Most common combinations are scenery (1) and traffic (2) as well as scenery (2) and traffic (3).\n\n\nCorrelation Analysis\nA correlation matrix comparing mileage, sentiment, and the three ratings showed no strong correlations. However, sentiment and terrain rating had the highest correlation, suggesting that challenging terrains might lead to more negative sentiments."
  },
  {
    "objectID": "technical-details/eda/main.html#multivariate-analysis",
    "href": "technical-details/eda/main.html#multivariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\nTo identify potential clusters, we visualized distance vs. sentiment, color-coded by the scenery score and annotated with the loop variable (loop vs. non-loop trails). The scatter plot revealed: - No clear clusters based on distance and sentiment. - Scenery scores often grouped closely, with no discernible differences between loop and non-loop trails."
  },
  {
    "objectID": "technical-details/eda/main.html#statistical-tests",
    "href": "technical-details/eda/main.html#statistical-tests",
    "title": "Exploratory Data Analysis",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nWe conducted a series of statistical tests to analyze the relationships between numerical, binary, and categorical features in our dataset. Below, we outline the methodology and results for each type of test:\n\nKolmogorov-Smirnov Test\nTo assess the normality of the numerical variables (mileage and sentiment), we used the Kolmogorov-Smirnov test. The results indicated that both variables appear to be normally distributed: - Sentiment: p-value ≈ 0.198 - Mileage: p-value ≈ 0.091\nSince the p-values are greater than 0.05, we fail to reject the null hypothesis, concluding that both variables are likely normally distributed.\n\n\n\nt-Tests (Continuous vs. Binary Variables)\nWe performed t-tests to compare the means of numerical variables (sentiment and mileage) across each binary variable in the dataset. For every combination: - The p-values were greater than 0.05. - This indicates that we fail to reject the null hypothesis, suggesting no significant differences in means between the numerical variables from the lens of the the binary features.\n\n\n\nChi-Squared Tests (Categorical Features)\nWe used chi-squared tests to examine the independence between all pairs of categorical variables (traffic, terrain, scenery, state1, and state2). The results revealed statistically significant associations for the following pairs: - Traffic vs. Terrain: p-value ≈ 0.039 - Terrain vs. State1: p-value ≈ 0.033 - State1 vs. State2: p-value ≈ 5.61e-11\nFor these pairs, we reject the null hypothesis at a significance level of 0.05, concluding that these categorical variables are not independent. All other pairs yielded p-values greater than 0.05, indicating no evidence of dependence.\n\n\n\nANOVA Tests (Continuous vs. Categorical Variables)\nWe conducted ANOVA tests to compare the means of continuous variables (sentiment and mileage) across the levels of categorical variables. Notable results include: - Sentiment across Terrain Levels: p-value ≈ 0.028 - This suggests a significant difference in sentiment means across terrain categories, leading us to reject the null hypothesis. - Sentiment across Traffic Levels: p-value ≈ 0.059 - Although this result is close, it does not meet the 0.05 significance threshold.\nFor all other tests, the p-values were greater than 0.05, indicating no significant differences in means across the levels of the binary or categorical variables.\n\n\n\nStatistical Summary\n\nNormality (Kolmogorov-Smirnov): Both sentiment and mileage are likely normally distributed.\nt-Tests: No significant differences in means between numerical and binary variables.\nChi-Squared Tests: Significant associations observed for traffic vs. terrain, terrain vs. state1, and state1 vs. state2.\nANOVA: Significant differences in sentiment across terrain levels; no other significant results.\n\nThese findings provide insights into the relationships within the dataset and guide further analysis."
  },
  {
    "objectID": "technical-details/eda/main.html#key-insights",
    "href": "technical-details/eda/main.html#key-insights",
    "title": "Exploratory Data Analysis",
    "section": "Key Insights",
    "text": "Key Insights\n\nSentiment is generally positive, with a bimodal distribution.\nMost trails are relatively short (10-120 miles), though a few exceed 175 miles.\nTerrain ratings are diverse, but traffic and scenery ratings have narrower ranges.\nNo strong correlations exist between numerical or categorical variables.\nDistinct clusters are not evident based on distance, sentiment, or scenery."
  },
  {
    "objectID": "technical-details/eda/main.html#conclusion-and-next-steps",
    "href": "technical-details/eda/main.html#conclusion-and-next-steps",
    "title": "Exploratory Data Analysis",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\nThe EDA revealed limited patterns for effectively grouping trails. While the correlation matrix and scatter plots showed minimal relationships, the insights into distributions and rating combinations inform our understanding of the data. These findings will guide the next steps: - Modeling: Explore whether machine learning can identify subtle patterns. - Feature Engineering: Consider different variable relationships or transformations to improve clustering potential.\nAll visualizations, including heatmaps, bar charts, and scatter plots, as well as statistical testing outputs, can be found in the Code section below."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/eda/overview.html",
    "href": "technical-details/eda/overview.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "With our processed data, we created numerous visualizations to uncover the underlying patterns in the bike trail dataset. Most of the analyses focused on univariate relationships, though we also explored some bivariate relationships, particularly between trail ratings. Additionally, we examined the potential for clustering by comparing mileage with sentiment, using the trail’s scenery score as a color-coded feature."
  },
  {
    "objectID": "technical-details/eda/overview.html#overview",
    "href": "technical-details/eda/overview.html#overview",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "With our processed data, we created numerous visualizations to uncover the underlying patterns in the bike trail dataset. Most of the analyses focused on univariate relationships, though we also explored some bivariate relationships, particularly between trail ratings. Additionally, we examined the potential for clustering by comparing mileage with sentiment, using the trail’s scenery score as a color-coded feature."
  },
  {
    "objectID": "technical-details/eda/overview.html#univariate-analysis",
    "href": "technical-details/eda/overview.html#univariate-analysis",
    "title": "DSAN-5000: Project",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\n\nNumerical Variables\nWe analyzed the distributions of numerical variables, particularly mileage and sentiment: - Mileage: A discrete variable, categorized into bins for easier analysis. The distribution shows that most trails fall within the 10-120 mile range, with a slight right skew indicating a few longer trails (up to 175 miles). - Sentiment: A continuous variable analyzed using Kernel Density Estimation. The distribution reveals that most sentiments are positive, concentrated within the 0-0.8 range. The bimodal nature of the distribution indicates peaks around 0 and 0.6.\n\n\nCategorical Variables\nBar charts helped us visualize the distributions of trail ratings—terrain, traffic, and scenery: - Terrain: The only rating using the full range (1-5). Most trails are rated 3, with fewer trails rated 4 or 5. - Traffic: Ranges from 1-4, with most trails rated 2 or 3. - Scenery: Restricted to scores of 1-3, with the majority of trails rated 1 or 2."
  },
  {
    "objectID": "technical-details/eda/overview.html#bivariate-analysis",
    "href": "technical-details/eda/overview.html#bivariate-analysis",
    "title": "DSAN-5000: Project",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nHeatmaps\nWe explored relationships between trail ratings using heatmaps: - Terrain vs. Scenery: The combination of terrain (3) and scenery (1) is the most frequent. - Terrain vs. Traffic: The most common combination is when both ratings are 3. - Traffic vs. Scenery: Most common combinations are scenery (1) and traffic (2) as well as scenery (2) and traffic (3).\n\n\nCorrelation Analysis\nA correlation matrix comparing mileage, sentiment, and the three ratings showed no strong correlations. However, sentiment and terrain rating had the highest correlation, suggesting that challenging terrains might lead to more negative sentiments."
  },
  {
    "objectID": "technical-details/eda/overview.html#multivariate-analysis",
    "href": "technical-details/eda/overview.html#multivariate-analysis",
    "title": "DSAN-5000: Project",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\nTo identify potential clusters, we visualized distance vs. sentiment, color-coded by the scenery score and annotated with the loop variable (loop vs. non-loop trails). The scatter plot revealed: - No clear clusters based on distance and sentiment. - Scenery scores often grouped closely, with no discernible differences between loop and non-loop trails."
  },
  {
    "objectID": "technical-details/eda/overview.html#statistical-tests",
    "href": "technical-details/eda/overview.html#statistical-tests",
    "title": "DSAN-5000: Project",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nWe conducted a series of statistical tests to analyze the relationships between numerical, binary, and categorical features in our dataset. Below, we outline the methodology and results for each type of test:\n\nKolmogorov-Smirnov Test\nTo assess the normality of the numerical variables (mileage and sentiment), we used the Kolmogorov-Smirnov test. The results indicated that both variables appear to be normally distributed: - Sentiment: p-value ≈ 0.198 - Mileage: p-value ≈ 0.091\nSince the p-values are greater than 0.05, we fail to reject the null hypothesis, concluding that both variables are likely normally distributed.\n\n\n\nt-Tests (Continuous vs. Binary Variables)\nWe performed t-tests to compare the means of numerical variables (sentiment and mileage) across each binary variable in the dataset. For every combination: - The p-values were greater than 0.05. - This indicates that we fail to reject the null hypothesis, suggesting no significant differences in means between the numerical variables from the lens of the the binary features.\n\n\n\nChi-Squared Tests (Categorical Features)\nWe used chi-squared tests to examine the independence between all pairs of categorical variables (traffic, terrain, scenery, state1, and state2). The results revealed statistically significant associations for the following pairs: - Traffic vs. Terrain: p-value ≈ 0.039 - Terrain vs. State1: p-value ≈ 0.033 - State1 vs. State2: p-value ≈ 5.61e-11\nFor these pairs, we reject the null hypothesis at a significance level of 0.05, concluding that these categorical variables are not independent. All other pairs yielded p-values greater than 0.05, indicating no evidence of dependence.\n\n\n\nANOVA Tests (Continuous vs. Categorical Variables)\nWe conducted ANOVA tests to compare the means of continuous variables (sentiment and mileage) across the levels of categorical variables. Notable results include: - Sentiment across Terrain Levels: p-value ≈ 0.028 - This suggests a significant difference in sentiment means across terrain categories, leading us to reject the null hypothesis. - Sentiment across Traffic Levels: p-value ≈ 0.059 - Although this result is close, it does not meet the 0.05 significance threshold.\nFor all other tests, the p-values were greater than 0.05, indicating no significant differences in means across the levels of the binary or categorical variables.\n\n\n\nStatistical Summary\n\nNormality (Kolmogorov-Smirnov): Both sentiment and mileage are likely normally distributed.\nt-Tests: No significant differences in means between numerical and binary variables.\nChi-Squared Tests: Significant associations observed for traffic vs. terrain, terrain vs. state1, and state1 vs. state2.\nANOVA: Significant differences in sentiment across terrain levels; no other significant results.\n\nThese findings provide insights into the relationships within the dataset and guide further analysis."
  },
  {
    "objectID": "technical-details/eda/overview.html#key-insights",
    "href": "technical-details/eda/overview.html#key-insights",
    "title": "DSAN-5000: Project",
    "section": "Key Insights",
    "text": "Key Insights\n\nSentiment is generally positive, with a bimodal distribution.\nMost trails are relatively short (10-120 miles), though a few exceed 175 miles.\nTerrain ratings are diverse, but traffic and scenery ratings have narrower ranges.\nNo strong correlations exist between numerical or categorical variables.\nDistinct clusters are not evident based on distance, sentiment, or scenery."
  },
  {
    "objectID": "technical-details/eda/overview.html#conclusion-and-next-steps",
    "href": "technical-details/eda/overview.html#conclusion-and-next-steps",
    "title": "DSAN-5000: Project",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\nThe EDA revealed limited patterns for effectively grouping trails. While the correlation matrix and scatter plots showed minimal relationships, the insights into distributions and rating combinations inform our understanding of the data. These findings will guide the next steps: - Modeling: Explore whether machine learning can identify subtle patterns. - Feature Engineering: Consider different variable relationships or transformations to improve clustering potential.\nAll visualizations, including heatmaps, bar charts, and scatter plots, as well as statistical testing outputs, can be found in the Code section below."
  },
  {
    "objectID": "technical-details/unsupervised-learning/overview.html",
    "href": "technical-details/unsupervised-learning/overview.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/overview.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/overview.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/overview.html#what-to-address",
    "href": "technical-details/unsupervised-learning/overview.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Analyzing Bike Routes in Washington, DC:",
    "section": "",
    "text": "This report is designed for a non-technical audience (e.g., the general public, executives, marketing teams, or clients), focusing on high-level insights, actionable results, and visualizations to convey the impact without requiring technical knowledge. The goal is to highlight how a model affects business strategy or revenue without diving into complex methods."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Analyzing Bike Routes in Washington, DC:",
    "section": "Guidelines for Creating a Good Narrative",
    "text": "Guidelines for Creating a Good Narrative\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Analyzing Bike Routes in Washington, DC:",
    "section": "Report Content",
    "text": "Report Content\nThese are just examples, you can use any structure that is suitable for your project.\n\nCase-1: Academic-Oriented Projects\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\nCase-2: Business-Oriented Projects\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Analyzing Bike Routes in Washington, DC:",
    "section": "Final Tips",
    "text": "Final Tips\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#introduction",
    "href": "report/report.html#introduction",
    "title": "Analyzing Bike Routes in Washington, DC:",
    "section": "Introduction",
    "text": "Introduction\nGoing to a new city can be overwhelming, especially when considering all of the new traditions, attractions, and outdoor activities that come with it. For avid biking fans, this stress is further amplified when trying to find new bike paths to explore. Which bike paths offer the best scenery around the city? Which paths are best for getting to work, going to the grocery store, or going somewhere else important? All of these are natural questions that come to mind when thinking about the biking scene of a new city. Even for those not passionate about biking, companies like Lime, Capital Bikeshare, and others provide accessible eBikes for those looking to get off their feet. Biking, both as a hobby and as a simple mode of transportation, is an essential part of a city’s culture. Not only that, it helps lower carbon emissions, aiding in the environmental footprint a city has. Undoubtedly, biking is an essential activity in the lives of many, especially those living in Washington, DC. It can be a daunting task to look at all bike trails in the area to find ones that best fit your goals. This study attempts to rectify that concern by clustering bike paths based on their inherent qualities. Specifically, we focus on three qualities to cluster the bike paths effectively - difficulty of terrain, the amount of traffic the bike path sees, and the scenery of the bike path. With these characteristics, we hope to group similar bike paths together, allowing the public to see which paths are best for their needs."
  },
  {
    "objectID": "report/report.html#objective",
    "href": "report/report.html#objective",
    "title": "Analyzing Bike Routes in Washington, DC:",
    "section": "Objective",
    "text": "Objective\nOur objective is to group similar bike paths together based on three discrete variables - terrain, traffic, and scenery."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Use this page to track your progress and keep a log of your contributions to the project, please update this each time you work on your project, it is generally a good habit to adopt.\nIf you are working as a team, at the end, you can duplicate the project and add it to your individual portfolio websites. If you do, you MUST retain attribution to your teammates. Removing attribution would constitute plagiarism."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with GPT\nwrite a technical methods sections for K-means\nwrite a technical methods sections for PCA\n\n… etc"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nGentry Lamb\nWeekly project contribution log:\n11-21-2024\n\nCoordinate with team member to set up first project meeting\n\n11-25-2024\n\nBegin working on data collection and researching use of Selenium\n\n12-01-2024\n\nWrite data collection code\n\n12-06-2024\n\nWrite literature review and put together the landing page\n\n12-07-2024\n\nUpdate data collection code to include distances\nWrite data cleaning code\nStart writing EDA code\n\n12-08-2024\n\nComplete writing EDA code"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2:",
    "text": "Member-2:\nChase Clemence\nWeekly project contribution log:\n11-21-2024\n\nAttend first group meeting\n\n12-06-2024\n\nComplete literature review and put together the landing page\n\n12-07-2024\n\nWrote technical details for data collection\nRefined the landing page to include more information\nWrote technical details for data cleaning\n\n12-08-2024\n\nWrote technical details for EDA\nCoded up a lot of the supervised learning portion\n\n12-08-2024\n\nFinished the code for supervised learning"
  }
]