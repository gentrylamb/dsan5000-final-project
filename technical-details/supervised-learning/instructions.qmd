# Supervised Learning - Leveraging Linear Regression and Decision Trees

In this analysis, we explore the performance of regression and classification decision tree models to understand the intricate relationships within our dataset. We found that the regression tree outperformed the linear regression model, likely because decision trees do not assume any specific functional form or distribution, whereas linear regression attempts to fit a straight line through the data, which may not be suitable for complex relationships. Decision trees, by learning simple decision rules from features, are better equipped to capture the underlying patterns in the bike path dataset.

## Data Preprocessing

### 1. **Normalization or Standardization**
   - We applied z-score normalization to the mileage feature to scale it appropriately with other features.

### 2. **Feature Selection or Extraction**
   - Given the limited size of the dataset, all available features were kept, with no extraction performed.

### 3. **Encoding Categorical Variables**
   - Categorical features were one-hot encoded to convert them into a binary format suitable for modeling.

## Training & Testing Strategy

### 1. **Split Methods**
   - We used the `train_test_split` function from `sklearn` to divide the dataset into training and testing sets, applying a standard 80/20 split.

### 2. **Dataset Proportions**
   - The dataset was divided such that 80% was used for training, and 20% was reserved for testing.

## Model Selection

### 1. **Model Rationale**
   - Decision tree models were selected due to their flexibility in capturing complex, non-linear relationships. We chose regression trees for continuous targets and classification trees for categorical targets.

### 2. **Overview of Algorithms**
   - **Linear Regression**: A parametric model used as a baseline for regression tasks.
   - **Regression Tree**: A non-parametric model that uses decision rules to predict continuous outcomes.
   - **Binary Classification Tree**: A model for predicting binary outcomes, such as whether a bike path is a loop or not.
   - **Multiclass Classification Tree**: A model used to predict more than two classes, applied to the 'state1' variable.

## Linear Regression

We began with linear regression as a baseline to understand how well a simple linear model could fit the data. Linear regression assumes a linear relationship between the target ('sentiment') and the input features. We evaluated the model using two metrics:

- **RMSE (Root Mean Squared Error)**: Measures the error between predicted and actual values, with a lower value being desirable.
- **R²**: Represents the proportion of variance explained by the model, where higher values indicate better fits.

For this analysis, we obtained:
- RMSE: 0.14
- R²: 0.03

Although the RMSE is low, the R² value indicates a poor fit, suggesting that a linear model is insufficient for capturing the underlying relationships in the data. We decided to proceed with more complex models.

## Regression Tree

Next, we applied a regression tree, which is non-parametric and can dynamically fit the data by learning decision rules. We fine-tuned the model by testing various `max_depth` hyperparameters, which control the depth of the tree. We found that a `max_depth` of 2 minimized the testing error while avoiding overfitting.

- **Optimal hyperparameter**: max_depth = 2
- **Training RMSE**: 0.11
- **Testing RMSE**: 0.048 (lower than the linear regression model)

The regression tree significantly outperformed linear regression in terms of RMSE, indicating that the model was better suited to capture the underlying patterns in the data.

## Binary Classification Tree

After the regression task, we focused on a binary classification problem: predicting whether a bike path is a loop (1) or not (0). Given the poor R² from the linear regression, we chose to avoid logistic regression and instead used a decision tree for classification. We tuned the `max_depth` hyperparameter and found that a value of 7 yielded the best performance.

- **Optimal hyperparameter**: max_depth = 7
- **Accuracy**: 78%
- **Confusion Matrix**:
  - Negative recall: 100%
  - Negative precision: 75%
  - Positive recall: 33%
  - Positive precision: 100%

The model performed well in terms of accuracy, but the confusion matrix revealed that it frequently predicted the non-loop class (0), likely due to class imbalance in the dataset. This bias resulted in a low positive recall.

## Multiclass Classification Tree

Finally, we aimed to predict the 'state1' variable, which represents multiple categories. We evaluated the model by testing various `max_depth` values to find the optimal setting. We selected `max_depth = 2` based on its performance.

- **Optimal hyperparameter**: max_depth = 2
- **Test Accuracy**: 44%

Although the model outperformed random guessing, with an accuracy of 44%, it was not highly predictive. The confusion matrix indicated that the model tended to predict the most prevalent class correctly, but struggled with the other categories.

## Discussion

### 1. **Result Interpretation**
   - The regression tree emerged as the best-performing model, with a testing RMSE of 0.048, which indicates a good fit. However, further testing with additional data would be important for validating its robustness.
   - The binary classification tree performed well overall, with 78% accuracy, but the confusion matrix highlighted issues with class imbalance. The model heavily favored predicting non-loop paths, which reduced its utility in predicting the loop class.
   - The multiclass classification model achieved an accuracy of 44%, which is marginally better than guessing randomly. It often predicted the most common class, but failed to predict the other categories accurately. More data and improved feature engineering would likely enhance the performance of this model.

### 2. **Model Performance Comparison**
   - Among all models tested, the regression tree offered the best performance with the lowest RMSE. The binary classification tree showed promising accuracy but suffered from class imbalance issues. The multiclass model's performance was the weakest, highlighting the need for better predictive features or more data.

### 3. **Insights Gained**
   - The regression tree is the most reliable model for this dataset, as it can better capture non-linear relationships. For classification tasks, particularly binary classification, addressing class imbalance and tuning the model further is crucial. The multiclass model, while slightly better than random, would benefit from more data and better feature selection to improve predictive accuracy.
